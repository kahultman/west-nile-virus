---
title: "WNV Project"
author: "Keith Hultman"
date: "12/4/2016"
output:
  html_document:
    TOC: yes
    code_folding: hide
    fig_caption: yes
    toc_depth: 2
  pdf_document:
    toc_depth: '2'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '../')

source("../src/functions.R")
# source("../src/01_weather.R")
# source("../src/02_train.R")
# source("../src/03_test.R")
# source("../src/05_combine_to_train.R")
# source("../src/06_geo_cluster.R")
```

# West Nile Virus Prediction

This project will attempt to predict the likelihood of a positive test for West Nile Virus in mosquitos captured using traps around Chicago, IL. The data sets originate from the Kaggle competition, [West Nile Virus Prediction](https://www.kaggle.com/c/predict-west-nile-virus). 

This analysis will use an iterative approach to finding a production model for West Nile forecasting. There are several data science iterative methods, and I will use the [CRISP](./images/CRISP.png) model for organizing this project. 

# CRISP Iteration 1

## Business Understanding 

Kaggle supplies five data sets for the competition, three of which can be used in model training. The train.csv file includes several variables associated with each West Nile Virus test including the trap id, geo-coordinates, date, species of mosquito present, number of mosquitos present, and a binary variable indicating the presence or absence of West Nile Virus (WnvPresent), our target variable. The test.csv contains all of the same features as the train.csv file except the number of mosquitos present and the target WnvPresent variable. These data sets were obtained from West Nile Virus testing period between 2007 and 2014 with the training set containing the odd years and the test set containing the even years. The weather.csv file contains historic weather data collected at the O'Hare and Midway airports concurrent with the mosquito testing time period. The spray.csv file includes the date and location of chemical spraying conducted by the city during 2011 and 2013. Since this spray data set would only have a large impact during one of the training years, I did not include the spray data in any of my models. 

## Data Understanding 

One of the largest factors in whether West Nile Virus is present or not is the number of mosquitos found in each trap. However, this variable is not present in the test set, and will need to be imputed from other variables before a final model can be fit to the presence of West Nile. 

```{r}
suppressMessages(library(tidyverse))
load("./data/train.RData")
load("./data/test.RData")

ggplot(train, aes(x=WnvPresent, y=NumMosquitos)) + geom_boxplot() + ggtitle("Number of mosquitos in West Nile Virus negative and positive traps") + xlab("West Nile Virus present in trap") + ylab("Number of mosquitos in trap")
```

Based on this initial data exploration, I will first try and develop a model that predicts the number of mosquitos in each trap and then use that predicted value as a variable in a model that predicts whether West Nile is present or absent. For predicting the number of mosquitos, I will primarily be using weather variables and weekly historic averages. 

```{r}
ggplot(train, aes(x=Tavg, y=NumMosquitos)) + geom_jitter() + geom_smooth(method=lm) + ggtitle("Number of mosquitos vs average daily temperature") + xlab("Average daily temperature, degree F") + ylab("Number of mosquitos per trap")

```




## Data Preparation 



## Modeling 1a: Linear regression to predict mosquito population

For this first round of modeling, I will use linear regression to predict the number of mosquitos using weather data and historical averages per week. I expect that higher average temperatures and greater moisture should have a positive effect on the mosquito population.

Then I will use logistic regression to estimate the probability of the presence of West Nile Virus. 

```{r Mosquito linear regression}
lm_variables <- c("Tmax", 
                      "Tmin",
                      "Tavg",
                      "DewPoint",
                      "WetBulb",
                      "Sunrise",
                      "Sunset",
                      "PrecipTotal",
                      "ResultSpeed",
                      "ma3",
                      "ma5",
                      "ma10",
                      "precip3d",
                      "precip5d",
                      "precip10d")

lm_variables <- paste(lm_variables, collapse = "+")
fmla <- paste("NumMosquitos", lm_variables, sep = "~")

lm1 <- lm(fmla, data = train)
train$lmNumMosq <- predict(lm1, newdata = train)
test$lmNumMosq <- predict(lm1, newdata = test)

summary(lm1)
```

## Evaluate linear model for predicting mosquito number

The R^2 of our first model is 0.11, meaning that only 11% of observed mosquito population variation is explained by our weather model.To further evaluate the linear regression model, we can examine the residual errors against actual numbers in the training data. 

```{r lm1 residual plot}
#ggplot(data = train, aes(x=lmNumMosq, y=(NumMosquitos - lmNumMosq))) + geom_point(alpha = 0.05)
ggplot(data = train, aes(x=lmNumMosq, y=(NumMosquitos - lmNumMosq))) + geom_point(alpha = 0.05) + xlab("Predicted number of Mosquitos in training set") + ylab("Residual error (Actual - Predicted)") + ggtitle("Residual plot for estimating Mosquito number using linear model on weather data") + stat_smooth(method=lm)

```

The residual plot appears to show bias in overestimating mosquitos at the low end and underestimating at the high end. However, the trend shows that this is indeed zero. Let's now include historical averages for each week in the year. We can also remove some variables that were not significantly informative. 

```{r mosquito linear regression 2}
weeklyAvgNumMosq <- train %>% group_by(Week) %>% summarise(WeekAvgMos = mean(NumMosquitos)) %>% ungroup()


train <- add_week_avg_mosq(train)
test <- add_week_avg_mosq(test)

lm_variables <- c("WeekAvgMos",
                      "Tavg",
                      "Tmin",
                      "WetBulb",
                      "ResultSpeed",
                      "ma5",
                      "ma10",
                      "precip5d")
lm_variables <- paste(lm_variables, collapse = "+")

fmla <- paste("NumMosquitos", lm_variables, sep = "~")

lm2 <- lm(fmla, data = train)
train$lm2NumMosq <- predict(lm2, newdata = train)
test$lm2NumMosq <- predict(lm2, newdata = test)
plot(fitted(lm2), residuals(lm2))
summary(lm2)
ggplot(data = train, aes(x=lmNumMosq, y=(NumMosquitos - lmNumMosq))) + geom_point(alpha = 0.05) + xlab("Predicted number of Mosquitos in training set") + ylab("Residual error (Actual - Predicted)") + ggtitle("Residual plot for estimating Mosquito number using linear model on weather data") + stat_smooth(method=lm)

```




## Mosquito number distribution

I next needed to examine the distribution of the Mosquito counts. 

```{r Mosquito Distribution}

par(mfcol = c(2,2))
hist(train$NumMosquitos)
qqnorm(train$NumMosquitos)
hist(log2(train$NumMosquitos))
qqnorm(log2(train$NumMosquitos))

par(mfcol = c(1,1))
```

The total number is capped at 50 mosquitos, and the distribution is centered close to zero with no negative values. Most observations are 1 mosquito. I attempted to use log2 transformation on mosquito number but that did not improve the regression. 

I will now try to group the number of mosquitos into bins, according to quartiles. These bins will be ordinal factors. Since we know that mosquito counts can never dip below zero, I will replace negative values for predicted number of mosquitos to zero before using this in the prediction for West Nile Virus. 


```{r Binning mosquito number}
train$NumMosQuartile <- with(train, cut(NumMosquitos, breaks=quantile(train$NumMosquitos, probs=seq(0,1, by=0.25), na.rm=TRUE), include.lowest=TRUE))

test$lmNumMosq[test$lmNumMosq <= 0] <- 1

test$NumMosQuartile <- with(test, cut(lmNumMosq, breaks=quantile(train$NumMosquitos, probs=seq(0,1, by=0.25), na.rm=TRUE), include.lowest=TRUE))

#sum(is.na(test$NumMosQuartile))
```

## Poisson Regression

The distribution of mosquitos looks closer to a capped Poisson distribution and this can be used to fit a general linear model. 

```{r Poisson regression}
glm1 <- glm(fmla, data = train, family = "poisson")
train$glm1NumMosq <- predict(glm1, newdata = train)
test$glm1NumMosq <- predict(glm1, newdata = test)

summary(glm1)

ggplot(data = train, aes(x=glm1NumMosq, y=(NumMosquitos - glm1NumMosq))) + geom_point(alpha = 0.05) + xlab("Predicted number of Mosquitos in training set") + ylab("Residual error (Actual - Predicted)") + ggtitle("Residual plot for estimating Mosquito number using Poisson regression model on weather data") + stat_smooth(method = lm)

```

This is much worse, as it only predicts up to 3 mosquitos per trap and appears to undercount mosquitos quite a bit. 

## Data understanding: Number of mosquitos

One important feature of how this data is collected and organized, is that the number of mosquitos in each trap is actually spread out over multiple rows. Therefore, we should collapse and combine multiple rows to find the number of mosquitos present in the environment. We also gain information from the separate rows mostly for the species information - some species are found in rows that are positive and not in rows that are negative from the same trap and this is useful information. 

```{r Real counts of Mosquitos and Poisson}
Mosq_number <- train %>% group_by(Trap, Date) %>% summarise(Mosq_count = sum(NumMosquitos), nrows = n())
train <- left_join(train, Mosq_number, by = c("Trap", "Date"))


fmla <- paste("Mosq_count", lm_variables, sep = "~")

glm1 <- glm(fmla, data = train, family = "poisson")
train$glm1NumMosq <- predict(glm1, newdata = train)
test$glm1NumMosq <- predict(glm1, newdata = test)

summary(glm1)

ggplot(data = train, aes(x=glm1NumMosq, y=(NumMosquitos - glm1NumMosq))) + geom_point(alpha = 0.05) + xlab("Predicted number of Mosquitos in training set") + ylab("Residual error (Actual - Predicted)") + ggtitle("Residual plot for estimating Mosquito number using Poisson regression model on weather data") + stat_smooth()

```

The number of rows in the training set for each trap-date combination is highly correlated with total mosquitos in the trap. This feature, the number of rows for trap-date, is present in the test set as well. 

```{r}
ggplot(data = train, aes(nrows, Mosq_count)) + geom_point() + ggtitle("Number of rows for each Trap reveals an estimate for total number of mosquitos")
```

This feature was used by winning competitors to estimate the number of mosquitos. However, I feel this is a bit of cheating for business question which involves predicting the probability of WNV for an entire year. In actual practice, we would not know how many 'trap'-ful amounts of mosquitos will be tested. 

```{r}


```



## WNV Distribution

Moving on to predicting our target variable, the presence of West Nile Virus (WnvPresent in the data set), I will first examine the distribution of the variable. 

```{r}
table(train$WnvPresent)

table(train$WnvPresent)[2] / sum(table(train$WnvPresent))
```

Finding WNV is rare, in only ~ 5% of cases. 


## Modeling 1b: Logistic regression to predict WNV infection

```{r}
lgm1 <- glm(WnvPresent ~ NumMosQuartile + ma10 + WetBulb + Tmin + Summer + Year, data = train, family = binomial(link = "logit"))
train$Wnv.glm.Pred <- predict(lgm1, newdata = train, type = "response")
test$NumMosquitos <- test$lmNumMosq
test$Wnv.glm.Pred <- predict(lgm1, newdata = test, type = "response")
summary(lgm1)

sum(is.na(test$Wnv.glm.Pred))

```

## Evaluation 

Now we can take a look at how well our model will perform based on where the cutoff for probability should be for predicting WnvPresent.
```{r}
suppressMessages(library(ROCR))
suppressMessages(library(pROC))

model.evaluate <- function(actual, predicted){
  pred <- prediction(predicted, actual)
  perf <- performance(pred, "tpr", "fpr")
  plot(perf)
  auc(actual, predicted)
}

model.evaluate(train$WnvPresent, train$Wnv.glm.Pred)
```

The AUC for the test set, after submitting the predicted probabilities to Kaggle was 0.700. Since this is a bit lower than the 0.814 AUC from the training set, this model is likely overfit to the training set. 

## Using number of rows as an estimate of mosquito number

```{r nrows Log Reg}
lgm2 <- glm(WnvPresent ~ nrows + ma10 + WetBulb + Tmin + Summer + Year, data = train, family = binomial(link = "logit"))
train$Wnv.glm2.Pred <- predict(lgm2, newdata = train, type = "response")


model.evaluate(train$WnvPresent, train$Wnv.glm2.Pred)


Mosq_number <- test %>% group_by(Trap, Date) %>% summarise(nrows = n())
test <- left_join(test, Mosq_number, by = c("Trap", "Date"))

test$Wnv.glm2.Pred <- predict(lgm2, newdata = test, type = "response")
summary(lgm2)

glm.submission2 <- select(test, Id, WnvPresent = Wnv.glm2.Pred)
write_csv(glm.submission2, path = "./data/glm.submission2.csv")
```

This gives an AUC of 0.711 on Kaggle.

# CRISP Iteration 2

The logistic regression model might be improved with using a boosting algorithm
 
```{r, eval=FALSE, include=FALSE}
suppressMessages(library(caret))

set.seed(300)
ctrl <- trainControl(method = "repeatedcv", 
                     number = 10, 
                     repeats = 3,
                     classProbs = TRUE, 
                     summaryFunction = twoClassSummary)

glmboost <- train(WnvPresent ~ NumMosquitos + precip10d + ma10 + Tmin + Summer + Year,
              data = train, 
              method = "glmboost",
              metric = "ROC",
              trControl = ctrl,
              tuneLength = 5,
              center = TRUE,
              family = Binomial(link = c("logit")))


ctrl <- trainControl(method = "repeatedcv", number = 10, savePredictions = TRUE)

lgm2 <- train(WnvPresent ~ NumMosquitos + precip10d + ma10 + Tmin + Summer + Year,  data=train, method="glm", family="binomial",
                 trControl = ctrl, tuneLength = 5)

train2 <- select(train, WnvPresent, NumMosquitos, precip10d, ma10, Tmin, Summer, Year)
summary(train2)
fitControl <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 10,
                           ## Estimate class probabilities
                           classProbs = TRUE,
                           ## Evaluate performance using 
                           ## the following function
                           summaryFunction = twoClassSummary)

set.seed(2014)

glmBoostModel <- train(WnvPresent ~ ., data=train, method = "glmboost", metric="ROC", trControl = fitControl, tuneLength=5, center=TRUE, family=Binomial(link = c("logit")))

```

## Data Understanding 2

The first model did not involve geographical information or any trap-specific information. Next, I wanted to visually examine the relationship between geography and West Nile presence. First I created a map of Wnv incidents as a density plot. Here, the density is based off of the overall positive count for each trap. 

 

```{r}
suppressMessages(library(ggmap))
chicago <- get_map("Chicago")
wnpositive <- filter(train, WnvPresent == TRUE)
load("./data/traps.RData")

ggmap(chicago) + geom_density2d(data = wnpositive, aes(x = Longitude, y = Latitude), size = 0.3) + 
  stat_density2d(data = wnpositive, aes(x = Longitude, y = Latitude, fill = ..level.., alpha = ..level..), size = 0.01, 
    bins = 16, geom = "polygon") + 
  scale_fill_gradient(name="Number of WNV+", low = "green", high = "red") + 
  scale_alpha(range = c(0, 0.3), guide = FALSE) + 
  ggtitle("Heat map of West Nile Virus positive traps") + 
  xlab("Longitude") + ylab("Latitude") +
  geom_point(data = traps, aes(x = Longitude, y = Latitude), shape = 4)
```

It's clear from the density map that there are hotspot traps where West Nile is more common than others, and this appears to influence nearby traps.  

Now that we have a good 2-dimensional view of the occurance of WNV, I next wanted to get a sense of how this pattern of West Nile Virus changed over time in relation to the geography. I did this by creating an animated version of this map using a new R package called gganimate. I chose to remove the background map of chicago to get a clearer visual of positive vs negative cases. Animating the map requires too much time to rebuild in the document, but the code can be found in the src folder called "07_animated_map.R"

![Animated map (not available in PDF document)](../images/mosq_map.gif)

After seeing how highly varied the density of WnvPresent traps were in the heat map and animated plots, my next set of models will try to incorporate this regional geographic information. 

There are various ways to incorporate geography. I could incorporate the trap id in the model or I could incorporate the longitude and latitude for very high precision. The longitude/latitude method would require a decision tree or other model that allows for breaking up or clustering the locations in some non-linear way. 

Instead, I am choosing to group the traps into clusters and include the cluster as a modeling parameter. The reasoning for this is that suspect that the true risk based on geography has a much lower actual precision than our ability to define latitude or longitude. 

## Data Preparation 2

To create clusters of traps I want to incorporate not only geographical information, but WNV rate as well. This will help to create breaks in my clusters that are informative for predicting future WNV rate. 

```{r}
trap_geo_hot <- train %>% group_by(Trap, Longitude, Latitude) %>% 
  summarise(number = n(), WnvTot = sum(WnvPresent)) %>% 
  mutate(WnvAvg = WnvTot/number) %>% 
  select(Trap, Latitude, Longitude, WnvAvg) %>% 
  arrange(Trap)

# Geographic + Hotspot clusters using k-means with k=6
set.seed(123)
geo.hot.cl <- kmeans(trap_geo_hot[,2:4], centers = 6)
trap_geo_hot$geo.hot.cl <- as.factor(geo.hot.cl$cluster)
#trap_geo_hot$geo.hot.cl <- as.factor(trap_geo_hot$geo.hot.cl)
#ggplot(trap_geo_hot, aes(Longitude, Latitude, color = geo.hot.cl)) + geom_point()

# Add Geographic hotspot clusters to trainset

train <- left_join(train, trap_geo_hot, by = c("Trap", "Longitude", "Latitude"))

save(train, file = "./data/train.RData")

test <- left_join(test, trap_geo_hot, by = c("Trap", "Longitude", "Latitude"))

save(test, file = "./data/test.RData")

ggplot(trap_geo_hot, aes(Longitude, Latitude, color = geo.hot.cl)) + geom_point()

ggmap(chicago) + geom_density2d(data = wnpositive, aes(x = Longitude, y = Latitude), size = 0.3) + 
  stat_density2d(data = wnpositive, aes(x = Longitude, y = Latitude, fill = ..level.., alpha = ..level..), size = 0.01, 
    bins = 16, geom = "polygon") + 
  scale_fill_gradient(name="Number of WNV+", low = "green", high = "red") + 
  scale_alpha(range = c(0, 0.3), guide = FALSE) + 
  ggtitle("Heat map of West Nile Virus positive traps") + 
  xlab("Longitude") + ylab("Latitude") +
  geom_point(data = trap_geo_hot, aes(x = Longitude, y = Latitude, shape = geo.hot.cl)) + scale_shape_discrete(name = "Trap Cluster")

```


## Modeling 2a: Decision Tree

C5.0 Decision trees

```{r, eval=FALSE, include=FALSE}
suppressMessages(library(C50))

train <- add_week_avg_mosq(train)
test <- add_week_avg_mosq(test)

trainC50df <- train %>% select(WnvPresent, Species, NumMosQuartile, Year, Week, WeekAvgMos, Tmin, WetBulb, PrecipTotal, ResultSpeed, ma10, precip3d, precip5d, precip10d)
trainC50df$WnvPresent <- as.factor(trainC50df$WnvPresent)
save(trainC50df, file = "./data/trainC50df.RData")

c50m <- C5.0(WnvPresent ~., data = trainC50df, trials = 1, costs = NULL)
trainpredict <- predict(c50m, newdata = train, type = "prob")


roc1 <- roc(train$WnvPresent, trainpredict[,2])
auc(roc1)


summary(c50m)

?predict
test$C50.Wnv.pred <- predict(c50m, newdata = test, type = "prob")

```

## Data Understanding 3

I next wanted to examine more closely the distribution of WNV presence throughout the season. For each Year-Week-Trap combination, compute the ratio of WNV presense to total tests. Then graph the distribution with Weeks as the dependent variable.

```{r}

YWT_ratio <- train %>% group_by(Year, Week, Trap) %>% summarise(WnvPresent_ratio = sum(WnvPresent)/(sum(WnvPresent)+sum(!WnvPresent)))
hist(YWT_ratio$WnvPresent_ratio)
ggplot(YWT_ratio, aes(x=Week, y=WnvPresent_ratio)) + geom_jitter() + ggtitle("Ratio of WNV over Weeks") + stat_smooth()


W_ratio <- train %>% group_by(Week) %>% summarise(WnvPresent_ratio = sum(WnvPresent)/(sum(WnvPresent)+sum(!WnvPresent)))
hist(W_ratio$WnvPresent_ratio)
ggplot(W_ratio, aes(x=Week, y=WnvPresent_ratio)) + geom_jitter() + ggtitle("Ratio of WNV over Weeks") + stat_smooth()



W_count <- train %>% group_by(Week) %>% summarise(WnvPresent_count = sum(WnvPresent))
ggplot(W_count, aes(x=Week, y=WnvPresent_count)) + geom_jitter() + ggtitle("Count of WNV over Weeks") + stat_smooth()

W_count_loess <- loess(WnvPresent_count ~ Week, data = W_count)
# plot residuals
plot(W_count_loess$residuals)

W_count_loess$fitted
```

## Modeling 3b: 

I would like to incorporate the above distribution, which looks approxiamately normal throughout the season. 

## Evaluation 1


## Modeling 4: Random Forest

```{r}
suppressMessages(library(randomForest))


```


# Deployment




```{r}
sessionInfo()
```

